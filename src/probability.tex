\section{Probability}

\formdesc{Assumptions}

\begin{center}
  \begin{tabular}{c}
    $0 \leq P(E) \leq 1$ \\
    $P(S) = 1$ \\
    $P(S) = \sum_{i=1}^{\infty} P(E_i)$ \\
  \end{tabular}
\end{center}

Sample space $S$ contains each event $E_i$, e.g., $E = \{$all outcomes in $S$ starting with a $3\}$

\hformbar



\formdesc{Unions and Intersections}

\begin{equation}
	E \cup F
\end{equation}

is the union of the two sets $E$ and $F$, i.e., the event where either $E$ or $F$ occurs. The intersection of two events, the outcomes in both $E$ and $F$ is

\begin{equation}
	E \cap F.
\end{equation}

\begin{center}
  \begin{tabular}{lc}
    Commutative  & $E \cup F = F \cup E$ \\
                 & $E \cap F = F \cap E$ \\
    Associative  & $(E \cup F) \cup G = E \cup (F \cup G) $ \\
                 & $(E \cap F) \cap G = E \cap (F \cap G)$ \\
    Distributive & $(E \cup F) \cap G = (E \cap G) \cap (F \cap G)$  \\
                 & $(E \cap F) \cup G = (E \cup G)(F \cup G)$
  \end{tabular}
\end{center}
\hformbar



\formdesc{Independent Events}



Two events are independent if knowing the outcome of one provides no useful information about the outcome of the other.
\hformbar


\formdesc{Mutually Exclusive Events}

\begin{equation}
   P(A \cap B) = 0
\end{equation}

\vspace{.63em}
are disjoint events, when $A$ and $B$ are mutually exclusive and there is no intersection---it is not possible for both to happen.
\hformbar



\formdesc{Union and Addition Rule}

\begin{equation}
    P(A \cup B) \equiv P(A \vee B) \equiv \{x: x \in A \vee x \in B\}
\end{equation}

\begin{center}
  \begin{tabular}{ll}
    Independent         & $P(A \cup B) = P(A) + P(B) - P(A \cap B)$   \\
    Mutually exclusive  & $P(A \cup B) = P(A) + P(B)$                 \\
  \end{tabular}
\end{center}

\hformbar



\formdesc{Intersection and Multiplication Rules}

\begin{equation}
    P(A \cap B) \equiv P(A \wedge B) \equiv \{x: x \in A \wedge x \in B\}
\end{equation}

\begin{center}
  \begin{tabular}{ll}
    Independent         & $P(A \cap B) = P(A) \cdot P(B)$   \\
    Mutually exclusive  & $P(A \cap B) = 0$                 \\
    Dependent           & $P(A \cap B) = P(A) \cdot P(B|A)$ \\
  \end{tabular}
\end{center}

\hformbar


\formdesc{Complement and Subtraction Rule}

\begin{eqnarray}
P(A') \equiv P(A^c) \equiv P(\neg A) &\equiv& \{x: x \notin A\} \equiv 1 - P(A) \\
P(A') &=& 1 - P(A)
\end{eqnarray}

Some implications:

\begin{center}
  \begin{tabular}{c}
    $P(A \cup A^c) = 1$ \\
    $P(A) = 1 - P(A^c)$ \\
    $P(A|B) = 1 - P(A^c | B)$ \\
  \end{tabular}
\end{center}
\hformbar




\formdesc{Conditional Probability Rule}

\begin{equation}
	P(A|B) = \frac{P(A \cap B)}{P(B)}
\end{equation} 

is the probability of the outcome of event A given condition B.
\hformbar



\formdesc{Bayes Theorem}

\begin{equation}
  P(A|B) = \frac{P(A) ~P(B|A)}{ P(A) ~ P(B|A) + P(A) ~ P(B|A)}
\end{equation}
\hformbar




\formdesc{The Fundamental Principle of Counting}

If an operation can be performed in $n_1$ ways, and if for each of these a second operation can be performed $n_2$ ways, and for each of the first two a third operation can be performed in $n_3$ ways, $\ldots$, then the sequence of $k$ operations can be performed in $n_1 n_2 \mdots n_k$ ways.

\hformbar



\formdesc{Factorial}

\begin{equation}
    n! = n (n-1) (n-2) (2) (1)
\end{equation}

with $0! = 1$.
\hformbar



\formdesc{Permutation}

A permutation $\sigma$ any finite set $A$ is a one-to-one mapping of $A$ onto itself. An element mapped to itself in the permutation is a \textit{fixed point}.

\subsection*{Example}

One permutation of the set $A = \{a, b, c\}$ is:

\begin{equation}
    \sigma = \begin{pmatrix}
                  a & b & c \\ 
                  b & c & a
             \end{pmatrix}
\end{equation}

where $a$ is sent to $b$, $b$ to $c$, and $c$ to $a$.
\hformbar


\formdesc{Permutations of $n$ elements}

\begin{equation}
    n!
\end{equation}

is the number of permutations of $n$ objects, that is, the number of arrangements of a set containing $n$ elements.
\hformbar



\formdesc{Permutations: $n$ taken $r$ at a time}

\begin{equation}
    \perm{n}{k}=\frac{n!}{(n-k)!}
\end{equation}

represents the number of permutations of $n$ distinct objects taken $r$ at a time.
\hformbar



\formdesc{Stirling's Approximation of $n!$}

\begin{equation}
    n^n e^{-n} \sqrt{2 \pi n}
\end{equation}

is a sequence asymptotically equal to $n!$.
\hformbar



\formdesc{Combinations}

\begin{equation}
    \comb{n}{j} = \begin{pmatrix} n \\ j \end{pmatrix} = \frac{n!}{j! (n - j)!}
\end{equation}

is the number of subsets of size $j$ that can be assembled given a set of $n$ elements, for integers $n$ and $j$ such that $0 < j < n$.
\hformbar



\formdesc{Bernoulli Trials Process}

A sequence of $n$ experiments such that

\begin{itemize}
    \item Each experiment has two possible outcomes, called \textit{success} and \textit{failure}
    \item The probability of $p$ of success of each experiment is the same for each, and is independent of previous experiments
\end{itemize}

The probability of failure is $q = 1 - p$.
\hformbar


\formdesc{Binomial Probabilities}

\begin{equation}
    b(n, p, j) = \comb{n}{j} p^j (1 - p)^{n - j}
\end{equation}

represents the probability that in $n$ Bernoulli trials there are exactly $j$ successes, given $n$ trials, and each trials' success rate is $p$.
\hformbar



\formdesc{Binomial Distribution}

\begin{equation}
    B \sim b(n, p, k)
\end{equation}

gives the probability of the number of successes $k$ in a sequence of Bernoulli trials with parameters $p$ and $n$.
\hformbar

PDF P. 107

\newpage
